{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import create_repo, login, HfApi\n",
    "from concurrent.futures import ThreadPoolExecutor, \\\n",
    "                               as_completed\n",
    "from tenacity import retry, stop_after_attempt, \\\n",
    "                     wait_random_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/faiq0913/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# set login credentials for Huggingface\n",
    "login('')\n",
    "\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Load and generate chain of thought for Boolq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '../datasets/boolq-english-train.jsonl',\n",
    "    '../datasets/boolq-english-val.jsonl',\n",
    "    '../datasets/openai-generated/boolq-malay-train-fixed.jsonl',\n",
    "    '../datasets/openai-generated/boolq-malay-val-fixed.jsonl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74f1a39daf049d69dad3f57921110a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96df441d74e24f3ea7172d1d561833a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47af53a56d0b439693a046ffe855b77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f2c28390264e99ad3b3440fc3369b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 25391\n"
     ]
    }
   ],
   "source": [
    "data_all = []\n",
    "\n",
    "for f in files:\n",
    "    with open(f) as fopen:\n",
    "        for d in tqdm(fopen):\n",
    "            d = json.loads(d)\n",
    "            data_all.append(d)\n",
    "\n",
    "print(f'Length of dataset: {len(data_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'do iran and afghanistan speak the same language',\n",
       " 'answer': 1,\n",
       " 'passage': 'Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.',\n",
       " 'language': 'English',\n",
       " 'split': 'train'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle when all retries fail\n",
    "def after_all_retries(retry_state):\n",
    "    print(retry_state.outcome.exception())\n",
    "    print(retry_state)\n",
    "    print(\"All retries failed. Moving on...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_random_exponential(min=1, max=60),\n",
    "        retry_error_callback=after_all_retries,\n",
    ")\n",
    "def generate_reasoning(sample):\n",
    "    prompt = f\"\"\"You were initially tasked with determining whether a particular\n",
    "        statement/question is factually/logically consistent (1) or not (0) based on a\n",
    "        given passage.\n",
    "\n",
    "        Passage (Input):\n",
    "        ```\n",
    "        {sample['passage']}\n",
    "        ```\n",
    "\n",
    "        Statement (Input):\n",
    "        ```\n",
    "        {sample['question']}\n",
    "        ```\n",
    "\n",
    "        Answer (Output):\n",
    "        ```\n",
    "        {sample['answer']}\n",
    "        ```\n",
    "\n",
    "        Provide step-by-step explanation for the output based on the Context/Statement given.\n",
    "        Please give your answer in {sample['language']}.\n",
    "        \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      max_completion_tokens=1024,\n",
    "      temperature=0.2,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in detecting factual inconsistencies and hallucinations.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'passage': sample['passage'],\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer'],\n",
    "        'language': sample['language'],\n",
    "        'split': sample['split'],\n",
    "        'reasoning': completion.choices[0].message.content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passage': 'In the canonical case, lawyers seeking admission must earn a Juris Doctor degree from a law school approved by the jurisdiction, and then pass a bar exam administered by it. Typically, there is also a character and fitness evaluation, which includes a background check. However, there are exceptions to each of these requirements. A lawyer who is admitted in one state is not automatically allowed to practice in any other. Some states have reciprocal agreements that allow attorneys from other states to practice without sitting for another full bar exam; such agreements differ significantly among the states.',\n",
       " 'question': 'do you have to have a college degree to take the bar exam',\n",
       " 'answer': 1,\n",
       " 'language': 'English',\n",
       " 'split': 'train',\n",
       " 'reasoning': 'To determine whether the statement \"do you have to have a college degree to take the bar exam\" is factually/logically consistent with the provided passage, we can analyze the information step-by-step:\\n\\n1. **Understanding the Passage**: The passage discusses the requirements for lawyers seeking admission to practice law, specifically mentioning the need for a Juris Doctor (JD) degree from an approved law school and passing a bar exam. It also mentions a character and fitness evaluation but does not explicitly state that a college degree is required to take the bar exam.\\n\\n2. **Identifying Key Points**:\\n   - A Juris Doctor degree is required to take the bar exam.\\n   - The passage does not mention the necessity of a college degree as a prerequisite for obtaining a JD.\\n   - It is implied that one must complete undergraduate education to enroll in law school (and thus obtain a JD), but this is not explicitly stated.\\n\\n3. **Analyzing the Statement**: The statement asks if a college degree is necessary to take the bar exam. Since the passage does not provide a direct answer to this question, we need to consider the implications:\\n   - Typically, to enroll in a law school and earn a JD, a college degree is required. Therefore, while the passage does not explicitly state that a college degree is required to take the bar exam, it is generally understood that one must have completed undergraduate education to pursue a JD.\\n\\n4. **Conclusion**: Given that the passage does not contradict the common understanding that a college degree is necessary to pursue a JD (and subsequently take the bar exam), we can conclude that the statement is consistent with the information provided in the passage.\\n\\nThus, the output is:\\n```\\n1\\n``` \\n\\nThis indicates that the statement is factually/logically consistent with the passage.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reasoning(data_all[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passage': \"A Wrinkle in Time adalah novel fantasi sains yang ditulis oleh penulis Amerika Madeleine L'Engle, pertama kali diterbitkan pada tahun 1962. Buku ini memenangi Anugerah Newbery, Anugerah Buku Sequoyah, dan Anugerah Rak Lewis Carroll, dan menjadi naib juara untuk Anugerah Hans Christian Andersen. Ia adalah buku pertama dalam Quintet Waktu L'Engle, yang mengikuti kisah Murrys dan Calvin O'Keefe.\",\n",
       " 'question': 'adakah a wrinkle in time sebuah cerita benar',\n",
       " 'answer': 0,\n",
       " 'language': 'Malay',\n",
       " 'split': 'train',\n",
       " 'reasoning': 'Untuk menentukan konsistensi fakta dari pernyataan \"adakah A Wrinkle in Time sebuah cerita benar\" berdasarkan petikan yang diberikan, kita perlu mengikuti langkah-langkah berikut:\\n\\n1. **Memahami Petikan**: Petikan menyatakan bahawa \"A Wrinkle in Time\" adalah novel fantasi sains yang ditulis oleh Madeleine L\\'Engle dan diterbitkan pada tahun 1962. Ia juga menyebut bahawa buku ini memenangi beberapa anugerah dan merupakan buku pertama dalam Quintet Waktu L\\'Engle.\\n\\n2. **Menganalisis Genre**: Dari petikan, jelas bahawa \"A Wrinkle in Time\" adalah sebuah novel fantasi sains. Genre ini biasanya melibatkan elemen yang tidak berdasarkan pada kenyataan atau fakta yang boleh dibuktikan, tetapi lebih kepada imaginasi dan kreativiti penulis.\\n\\n3. **Menilai Pernyataan**: Pernyataan yang ditanya adalah sama ada \"A Wrinkle in Time\" adalah sebuah cerita benar. Dalam konteks ini, \"cerita benar\" merujuk kepada naratif yang berdasarkan pada fakta atau kejadian yang benar-benar berlaku.\\n\\n4. **Kesimpulan**: Oleh kerana \"A Wrinkle in Time\" adalah sebuah novel dalam genre fantasi sains, ia tidak boleh dianggap sebagai \"cerita benar\". Ia adalah karya fiksyen yang dicipta oleh penulis dan tidak berdasarkan pada kenyataan.\\n\\n5. **Memberikan Jawapan**: Oleh itu, pernyataan tersebut tidak konsisten dengan fakta yang diberikan dalam petikan. Maka, jawapan yang tepat adalah 0, yang menunjukkan bahawa pernyataan itu tidak konsisten.\\n\\nKesimpulannya, output adalah:\\n```\\n0\\n```'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reasoning(data_all[14500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Use multithreading and call OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fdbc9d50e74ea98b84a9dfd0e8c6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_worker = 50\n",
    "\n",
    "for i in tqdm(range(0, len(data_all), max_worker)):\n",
    "    with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "        futures = {executor.submit(generate_reasoning, t): t for t in data_all[i: i + max_worker]}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                with open('../datasets/openai-generated/boolq-with-reasoning.jsonl', 'a') as final:\n",
    "                    json.dump(result, final)\n",
    "                    final.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/wanadzhar913/boolq-malay-with-chain-of-thought', endpoint='https://huggingface.co', repo_type='dataset', repo_id='wanadzhar913/boolq-malay-with-chain-of-thought')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_repo(\"wanadzhar913/boolq-malay-with-chain-of-thought\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7869d0c91d341ba88c598c62790a489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boolq-with-reasoning.jsonl:   0%|          | 0.00/55.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/wanadzhar913/boolq-malay-with-chain-of-thought/commit/d10139839e765a339d46c35f184c6e34fca1b695', commit_message='Upload boolq-with-reasoning.jsonl with huggingface_hub', commit_description='', oid='d10139839e765a339d46c35f184c6e34fca1b695', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/wanadzhar913/boolq-malay-with-chain-of-thought', endpoint='https://huggingface.co', repo_type='dataset', repo_id='wanadzhar913/boolq-malay-with-chain-of-thought'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj='../datasets/openai-generated/boolq-with-reasoning.jsonl',\n",
    "    path_in_repo='boolq-with-reasoning.jsonl',\n",
    "    repo_id=\"wanadzhar913/boolq-malay-with-chain-of-thought\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
